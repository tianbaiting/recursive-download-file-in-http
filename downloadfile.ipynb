{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b8b76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://*** fill the web you needed\"  \n",
    "DOWNLOAD_DIR = \"samurai_workshop_downloads\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "VISITED = set()\n",
    "\n",
    "def download_file(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, headers=HEADERS, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        filepath = os.path.join(DOWNLOAD_DIR, filename)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"  √ 已下载: {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  × 下载失败 {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_filename(title):\n",
    "    return re.sub(r'[\\\\/:*?\"<>|]', '', title).strip()\n",
    "\n",
    "def is_document_link(href):\n",
    "    return re.search(r'\\.(pdf|pptx?|docx?|xls[xm]?|zip)$', href, re.IGNORECASE)\n",
    "\n",
    "def get_absolute_url(base, link):\n",
    "    if link.startswith('http'):\n",
    "        return link\n",
    "    if link.startswith('/'):\n",
    "        return '/'.join(base.split('/')[:3]) + link\n",
    "    return base.rstrip('/') + '/' + link\n",
    "\n",
    "def crawl_recursive(url, depth=1, max_depth=3):\n",
    "    if depth > max_depth or url in VISITED:\n",
    "        return\n",
    "    VISITED.add(url)\n",
    "    print(f\"{'  '*depth}→ [{depth}] {url}\")\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        # 下载所有文档链接\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if is_document_link(href):\n",
    "                file_url = get_absolute_url(url, href)\n",
    "                original_filename = os.path.basename(file_url.split('?')[0])\n",
    "                title = clean_filename(a.get_text(strip=True)) or \"file\"\n",
    "                final_filename = f\"{title}_{original_filename}\"\n",
    "                download_file(file_url, final_filename)\n",
    "        # 递归查找所有子链接\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            link = a['href']\n",
    "            abs_link = get_absolute_url(url, link)\n",
    "            # 只递归 http(s) 链接，避免下载文件和外部站点\n",
    "            if abs_link.startswith('http') and not is_document_link(abs_link):\n",
    "                crawl_recursive(abs_link, depth+1, max_depth)\n",
    "    except Exception as e:\n",
    "        print(f\"{'  '*depth}× 访问失败 {url}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    crawl_recursive(BASE_URL, depth=1, max_depth=6)\n",
    "    print(\"\\n递归爬虫任务完成。请检查 'samurai_workshop_downloads' 目录。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9cde0d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "BASE_URL = \"  \"\n",
    "DOWNLOAD_DIR = \"samurai_workshop_downloads\"\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "VISITED = set()\n",
    "ALLOWED_PREFIXES = set()\n",
    "\n",
    "def download_file(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, headers=HEADERS, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        filepath = os.path.join(DOWNLOAD_DIR, filename)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        print(f\"  √ 已下载: {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  × 下载失败 {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def clean_filename(title):\n",
    "    return re.sub(r'[\\\\/:*?\"<>|]', '', title).strip()\n",
    "\n",
    "def is_document_link(href):\n",
    "    return re.search(r'\\.(pdf|pptx?|docx?|xls[xm]?|zip)$', href, re.IGNORECASE)\n",
    "\n",
    "def get_absolute_url(base, link):\n",
    "    if link.startswith('http'):\n",
    "        return link\n",
    "    if link.startswith('/'):\n",
    "        return '/'.join(base.split('/')[:3]) + link\n",
    "    return base.rstrip('/') + '/' + link\n",
    "\n",
    "def collect_allowed_prefixes(base_url):\n",
    "    \"\"\"收集第二层目录的绝对链接前缀\"\"\"\n",
    "    prefixes = set()\n",
    "    try:\n",
    "        resp = requests.get(base_url, headers=HEADERS, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            link = a['href']\n",
    "            abs_link = get_absolute_url(base_url, link)\n",
    "            # 只收集 http(s) 链接且不是文档文件\n",
    "            if abs_link.startswith('http') and not is_document_link(abs_link):\n",
    "                # 只收集属于 base_url 域名下的链接\n",
    "                if abs_link.startswith(base_url) or abs_link.startswith('/'.join(base_url.split('/')[:3])):\n",
    "                    # 只保留到目录层级（去掉参数和锚点）\n",
    "                    abs_link = abs_link.split('?')[0].split('#')[0].rstrip('/') + '/'\n",
    "                    prefixes.add(abs_link)\n",
    "    except Exception as e:\n",
    "        print(f\"收集第二层目录失败: {e}\")\n",
    "    return prefixes\n",
    "\n",
    "def crawl_recursive(url, depth=1, max_depth=6):\n",
    "    if depth > max_depth or url in VISITED:\n",
    "        return\n",
    "    # 限制递归范围：只允许在 ALLOWED_PREFIXES 下递归\n",
    "    if depth > 2:\n",
    "        in_allowed = False\n",
    "        for prefix in ALLOWED_PREFIXES:\n",
    "            if url.startswith(prefix):\n",
    "                in_allowed = True\n",
    "                break\n",
    "        if not in_allowed:\n",
    "            return\n",
    "    VISITED.add(url)\n",
    "    print(f\"{'  '*depth}→ [{depth}] {url}\")\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        # 下载所有文档链接\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if is_document_link(href):\n",
    "                file_url = get_absolute_url(url, href)\n",
    "                original_filename = os.path.basename(file_url.split('?')[0])\n",
    "                title = clean_filename(a.get_text(strip=True)) or \"file\"\n",
    "                final_filename = f\"{title}_{original_filename}\"\n",
    "                download_file(file_url, final_filename)\n",
    "        # 递归查找所有子链接\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            link = a['href']\n",
    "            abs_link = get_absolute_url(url, link)\n",
    "            # 只递归 http(s) 链接，避免下载文件和外部站点\n",
    "            if abs_link.startswith('http') and not is_document_link(abs_link):\n",
    "                crawl_recursive(abs_link, depth+1, max_depth)\n",
    "    except Exception as e:\n",
    "        print(f\"{'  '*depth}× 访问失败 {url}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global ALLOWED_PREFIXES\n",
    "    ALLOWED_PREFIXES = collect_allowed_prefixes(BASE_URL)\n",
    "    print(\"允许递归的第二层目录:\")\n",
    "    for p in ALLOWED_PREFIXES:\n",
    "        print(\"  \", p)\n",
    "    crawl_recursive(BASE_URL, depth=1, max_depth=6)\n",
    "    print(\"\\n递归爬虫任务完成。请检查 'samurai_workshop_downloads' 目录。\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
